"""
ComfyUI ç¼“å­˜åŠ é€Ÿå¼•æ“

è¿™ä¸ªæ¨¡å—å®ç°äº†çµæ´»è€Œé«˜æ•ˆçš„ç¼“å­˜ç®—æ³•ï¼Œé€šè¿‡ç›´æ¥æ›¿æ¢ transformer çš„ forward æ–¹æ³•
æ¥å®ç°æ¨ç†åŠ é€Ÿã€‚ç»è¿‡å®æµ‹ï¼Œè¿™ç§æ–¹æ³•åœ¨ FLUX ç­‰æ¨¡å‹ä¸Šèƒ½å®ç° 2x+ çš„åŠ é€Ÿæ•ˆæœã€‚

æ ¸å¿ƒé€»è¾‘ï¼š
1. æ‰¾åˆ° ComfyUI æ¨¡å‹ä¸­çš„ transformer ç»„ä»¶
2. æ›¿æ¢å…¶ forward æ–¹æ³•ä¸ºç¼“å­˜ç‰ˆæœ¬
3. æ”¯æŒå¤šç§ç¼“å­˜ç­–ç•¥ï¼ˆå›ºå®šè·³æ­¥ã€åŠ¨æ€è·³æ­¥ã€è‡ªé€‚åº”ç­‰ï¼‰
4. è·³è¿‡æ—¶è¿”å›ä¸Šæ¬¡ç»“æœ + å¾®é‡å™ªå£°ï¼ˆé˜²æ­¢ä¼ªå½±ï¼‰

æ–°å¢ç‰¹æ€§ï¼š
- å¯é…ç½®çš„ç¼“å­˜ç­–ç•¥
- åŠ¨æ€å‚æ•°è°ƒæ•´
- è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡
- æ ‡å‡† CacheDiT API å…¼å®¹æ€§
"""

import torch
import time
import functools
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass
import weakref


@dataclass
class CacheStrategy:
    """
    ç¼“å­˜ç­–ç•¥é…ç½®ç±»
    
    å®šä¹‰ç¼“å­˜è¡Œä¸ºçš„å„ç§å‚æ•°ï¼Œæ”¯æŒä¸åŒçš„åŠ é€Ÿç­–ç•¥ã€‚
    """
    skip_interval: int = 2          # è·³æ­¥é—´éš”ï¼ˆæ¯Næ­¥è·³è¿‡ä¸€æ¬¡ï¼‰
    warmup_steps: int = 3           # é¢„çƒ­æ­¥æ•°ï¼ˆå‰Næ­¥æ€»æ˜¯è®¡ç®—ï¼‰
    strategy_type: str = 'fixed'    # ç­–ç•¥ç±»å‹ï¼š'fixed', 'dynamic', 'adaptive'
    noise_scale: float = 0.001      # å™ªå£°ç¼©æ”¾å› å­
    enable_stats: bool = True       # æ˜¯å¦å¯ç”¨ç»Ÿè®¡
    debug: bool = False             # è°ƒè¯•æ¨¡å¼
    
    def should_skip(self, call_count: int) -> bool:
        """
        æ ¹æ®ç­–ç•¥å†³å®šæ˜¯å¦è·³è¿‡å½“å‰è°ƒç”¨
        
        Args:
            call_count: å½“å‰è°ƒç”¨æ¬¡æ•°
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥è·³è¿‡è®¡ç®—
        """
        if call_count <= self.warmup_steps:
            return False
            
        if self.strategy_type == 'fixed':
            # å›ºå®šé—´éš”è·³æ­¥
            return call_count % self.skip_interval == 0
        elif self.strategy_type == 'dynamic':
            # åŠ¨æ€è·³æ­¥ï¼šéšç€æ­¥æ•°å¢åŠ ï¼Œè·³æ­¥é¢‘ç‡æé«˜
            interval = max(1, self.skip_interval - (call_count - self.warmup_steps) // 10)
            return call_count % interval == 0
        elif self.strategy_type == 'adaptive':
            # è‡ªé€‚åº”è·³æ­¥ï¼šæ ¹æ®æ€§èƒ½è‡ªåŠ¨è°ƒæ•´ï¼ˆç®€åŒ–ç‰ˆï¼‰
            # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…çš„æ€§èƒ½ç›‘æ§æ¥åŠ¨æ€è°ƒæ•´
            return call_count % self.skip_interval == 0
        else:
            return False


@dataclass 
class ModelCacheState:
    """
    å•ä¸ªæ¨¡å‹çš„ç¼“å­˜çŠ¶æ€
    
    è·Ÿè¸ªæ¯ä¸ªæ¨¡å‹çš„ç¼“å­˜ç›¸å…³ä¿¡æ¯å’Œç»Ÿè®¡æ•°æ®ã€‚
    """
    model_id: str
    is_enabled: bool = True
    strategy: Optional[CacheStrategy] = None
    call_count: int = 0
    skip_count: int = 0
    compute_times: List[float] = None
    last_result: Optional[torch.Tensor] = None
    original_forward: Optional[callable] = None
    
    def __post_init__(self):
        if self.compute_times is None:
            self.compute_times = []


class EnhancedCache:
    """
    å¢å¼ºç‰ˆç¼“å­˜å®ç° - æ”¯æŒå¤šç§ç­–ç•¥å’ŒAPIå…¼å®¹æ€§
    
    æ ¸å¿ƒæ€æƒ³ï¼šåœ¨ diffusion æ¨¡å‹çš„è¿ç»­æ¨ç†æ­¥éª¤ä¸­ï¼Œç›¸é‚»æ­¥éª¤çš„è¾“å‡ºå¾€å¾€å¾ˆç›¸ä¼¼ï¼Œ
    å¯ä»¥é€šè¿‡è·³è¿‡éƒ¨åˆ†è®¡ç®—å¹¶é‡ç”¨ä¹‹å‰çš„ç»“æœæ¥å®ç°åŠ é€Ÿã€‚
    
    æ–°ç‰¹æ€§ï¼š
    - æ”¯æŒå¤šç§ç¼“å­˜ç­–ç•¥
    - å¯é…ç½®çš„å‚æ•°
    - è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
    - æ¨¡å‹çº§åˆ«çš„çŠ¶æ€ç®¡ç†
    - æ ‡å‡† CacheDiT API å…¼å®¹
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å¢å¼ºç¼“å­˜ç³»ç»Ÿ"""
        self.model_states: Dict[str, ModelCacheState] = {}  # æ¯ä¸ªæ¨¡å‹çš„çŠ¶æ€
        self.global_config: Dict[str, Any] = {}              # å…¨å±€é…ç½®
        self.model_refs = weakref.WeakKeyDictionary()        # å¼±å¼•ç”¨æ˜ å°„
        
        # å‘åå…¼å®¹çš„å…¨å±€ç»Ÿè®¡
        self.call_count = 0
        self.skip_count = 0
        self.compute_times = []
        
    def _get_model_id(self, model) -> str:
        """è·å–æ¨¡å‹çš„å”¯ä¸€æ ‡è¯†ç¬¦"""
        return f"{type(model).__name__}_{id(model)}"
    
    def _get_or_create_state(self, model, strategy: Optional[CacheStrategy] = None) -> ModelCacheState:
        """è·å–æˆ–åˆ›å»ºæ¨¡å‹çš„ç¼“å­˜çŠ¶æ€"""
        model_id = self._get_model_id(model)
        
        if model_id not in self.model_states:
            self.model_states[model_id] = ModelCacheState(
                model_id=model_id,
                strategy=strategy or CacheStrategy()
            )
            self.model_refs[model] = model_id
            
        return self.model_states[model_id]
    
    def enable_cache(self, model, strategy: Optional[CacheStrategy] = None):
        """
        ä¸ºæ¨¡å‹å¯ç”¨ç¼“å­˜ (æ–°çš„ API å…¼å®¹æ¥å£)
        
        Args:
            model: æ¨¡å‹å¯¹è±¡
            strategy: ç¼“å­˜ç­–ç•¥é…ç½®
        """
        state = self._get_or_create_state(model, strategy)
        state.is_enabled = True
        
        return self.patch_model(model, state)
    
    def disable_cache(self, model):
        """
        ä¸ºæ¨¡å‹ç¦ç”¨ç¼“å­˜ (æ–°çš„ API å…¼å®¹æ¥å£)
        
        Args:
            model: æ¨¡å‹å¯¹è±¡
        """
        model_id = self._get_model_id(model)
        
        if model_id in self.model_states:
            state = self.model_states[model_id]
            state.is_enabled = False
            
            # æ¢å¤åŸå§‹ forward æ–¹æ³•
            transformer = self._find_transformer(model)
            if transformer and state.original_forward:
                transformer.forward = state.original_forward
                if hasattr(transformer, '_original_forward'):
                    delattr(transformer, '_original_forward')
                print("âœ“ å·²æ¢å¤åŸå§‹ forward æ–¹æ³•")
        
    def patch_model(self, model, state: Optional[ModelCacheState] = None):
        """
        ä¸º ComfyUI æ¨¡å‹åº”ç”¨ç¼“å­˜è¡¥ä¸ (å¢å¼ºç‰ˆ)
        
        è¿™ä¸ªå‡½æ•°ä¼šï¼š
        1. åœ¨å¤æ‚çš„ ComfyUI æ¨¡å‹ç»“æ„ä¸­æ‰¾åˆ° transformer ç»„ä»¶
        2. ä¿å­˜åŸå§‹çš„ forward æ–¹æ³•
        3. æ›¿æ¢ä¸ºç¼“å­˜ç‰ˆæœ¬çš„ forward æ–¹æ³•
        
        Args:
            model: ComfyUI æ¨¡å‹å¯¹è±¡ï¼ˆé€šå¸¸æ˜¯ ModelPatcher ç±»å‹ï¼‰
            state: æ¨¡å‹ç¼“å­˜çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åº”ç”¨äº†ç¼“å­˜çš„æ¨¡å‹å¯¹è±¡
        """
        if state is None:
            state = self._get_or_create_state(model)
            
        print("=== ComfyUI ç¼“å­˜åŠ é€Ÿ (å¢å¼ºç‰ˆ) ===")
        print(f"   æ¨¡å‹ID: {state.model_id}")
        print(f"   ç­–ç•¥: {state.strategy.strategy_type}")
        print(f"   è·³æ­¥é—´éš”: {state.strategy.skip_interval}")
        print(f"   é¢„çƒ­æ­¥æ•°: {state.strategy.warmup_steps}")
        
        # ç¬¬ä¸€æ­¥ï¼šåœ¨ ComfyUI æ¨¡å‹ç»“æ„ä¸­æ‰¾åˆ° transformer
        transformer = self._find_transformer(model)
        if transformer is None:
            print("âŒ æœªèƒ½æ‰¾åˆ° transformer ç»„ä»¶")
            return model
            
        print(f"âœ“ æ‰¾åˆ° transformer: {type(transformer)}")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åº”ç”¨è¿‡ç¼“å­˜ï¼ˆé¿å…é‡å¤ä¿®æ”¹ï¼‰
        if hasattr(transformer, '_original_forward'):
            print("âš  æ¨¡å‹å·²ç»åº”ç”¨è¿‡ç¼“å­˜")
            return model
            
        # ç¬¬äºŒæ­¥ï¼šä¿å­˜åŸå§‹ forward æ–¹æ³•
        state.original_forward = transformer.forward
        transformer._original_forward = transformer.forward
        
        # ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºç¼“å­˜ç‰ˆæœ¬çš„ forward æ–¹æ³•
        def cached_forward(*args, **kwargs):
            """
            ç¼“å­˜ç‰ˆæœ¬çš„ forward æ–¹æ³• (å¢å¼ºç‰ˆ)
            
            æ”¯æŒå¤šç§ç¼“å­˜ç­–ç•¥å’Œè¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯æ”¶é›†ã€‚
            """
            if not state.is_enabled:
                # ç¼“å­˜è¢«ç¦ç”¨ï¼Œç›´æ¥è°ƒç”¨åŸå§‹æ–¹æ³•
                return state.original_forward(*args, **kwargs)
                
            state.call_count += 1
            self.call_count += 1  # å‘åå…¼å®¹
            call_id = state.call_count
            
            if state.strategy.debug:
                print(f"\nğŸ”„ Forward è°ƒç”¨ #{call_id} (æ¨¡å‹: {state.model_id})")
                print(f"   å‚æ•°æ•°é‡: {len(args)}")
                print(f"   å…³é”®å­—å‚æ•°: {list(kwargs.keys())}")
                
                # è®°å½•å¼ é‡ä¿¡æ¯ï¼ˆè°ƒè¯•æ¨¡å¼ï¼‰
                for i, arg in enumerate(args):
                    if isinstance(arg, torch.Tensor):
                        print(f"   å‚æ•°[{i}] å¼ é‡: {arg.shape}, è®¾å¤‡: {arg.device}, ç±»å‹: {arg.dtype}")
                
                # æ£€æŸ¥ transformer_optionsï¼ˆComfyUI ç‰¹æœ‰çš„å‚æ•°ä¼ é€’æ–¹å¼ï¼‰
                transformer_options = kwargs.get('transformer_options', {})
                print(f"   Transformer é€‰é¡¹: {list(transformer_options.keys())}")
            
            # æ ¸å¿ƒç¼“å­˜é€»è¾‘ï¼šæ ¹æ®ç­–ç•¥å†³å®šæ˜¯å¦è·³è¿‡è®¡ç®—
            should_skip = state.strategy.should_skip(call_id)
            
            if should_skip:
                state.skip_count += 1
                self.skip_count += 1  # å‘åå…¼å®¹
                
                if state.strategy.debug:
                    print(f"   ğŸš€ å°è¯•è·³è¿‡è®¡ç®— #{call_id}")
                
                # ä½¿ç”¨ç¼“å­˜ç»“æœï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                if state.last_result is not None:
                    if state.strategy.debug:
                        print(f"   âœ“ ä½¿ç”¨ç¼“å­˜ç»“æœï¼ˆæ¥è‡ªä¹‹å‰çš„è°ƒç”¨ï¼‰")
                    
                    # ä¸ºç¼“å­˜ç»“æœæ·»åŠ å¾®é‡å™ªå£°é˜²æ­¢å›¾åƒä¼ªå½±
                    if isinstance(state.last_result, torch.Tensor):
                        noise = torch.randn_like(state.last_result) * state.strategy.noise_scale
                        cached_result = state.last_result + noise
                        
                        if state.strategy.debug:
                            print(f"   ğŸ“Š ç¼“å­˜å‘½ä¸­ #{state.skip_count}")
                        return cached_result
            
            # æ­£å¸¸è®¡ç®—
            if state.strategy.debug:
                print(f"   ğŸ–¥ æ­£å¸¸è®¡ç®—è°ƒç”¨ #{call_id}")
            
            start_time = time.time()
            
            # è°ƒç”¨åŸå§‹çš„ forward æ–¹æ³•è¿›è¡Œå®é™…è®¡ç®—
            result = state.original_forward(*args, **kwargs)
            
            compute_time = time.time() - start_time
            
            if state.strategy.enable_stats:
                state.compute_times.append(compute_time)
                self.compute_times.append(compute_time)  # å‘åå…¼å®¹
            
            if state.strategy.debug:
                print(f"   â± è®¡ç®—è€—æ—¶: {compute_time:.3f}s")
            
            # ç¼“å­˜ç»“æœä¾›åç»­ä½¿ç”¨
            if isinstance(result, torch.Tensor):
                state.last_result = result.clone().detach()
                if state.strategy.debug:
                    print(f"   ğŸ’¾ å·²ç¼“å­˜ç»“æœ: {result.shape}")
            
            return result
        
        # ç¬¬å››æ­¥ï¼šæ›¿æ¢ forward æ–¹æ³•
        transformer.forward = cached_forward
        print("âœ“ Forward æ–¹æ³•å·²æ›¿æ¢ä¸ºå¢å¼ºç¼“å­˜ç‰ˆæœ¬")
        
        return model
        
    def _find_transformer(self, model):
        """
        åœ¨ ComfyUI æ¨¡å‹ç»“æ„ä¸­æŸ¥æ‰¾ transformer ç»„ä»¶
        
        ComfyUI çš„æ¨¡å‹ç»“æ„æ¯”è¾ƒå¤æ‚ï¼Œä¸åŒç±»å‹çš„æ¨¡å‹æœ‰ä¸åŒçš„åµŒå¥—ç»“æ„ï¼š
        - model.model.diffusion_model  # æœ€å¸¸è§
        - model.diffusion_model        # æ¬¡å¸¸è§  
        - model.transformer            # ç›´æ¥å¼•ç”¨
        
        Args:
            model: ComfyUI æ¨¡å‹å¯¹è±¡
            
        Returns:
            æ‰¾åˆ°çš„ transformer ç»„ä»¶ï¼Œå¤±è´¥è¿”å› None
        """
        
        print("ğŸ” æœç´¢ transformer ç»„ä»¶...")
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„è®¿é—®è·¯å¾„
        if hasattr(model, 'model') and hasattr(model.model, 'diffusion_model'):
            print("   æ‰¾åˆ°è·¯å¾„: model.model.diffusion_model")
            return model.model.diffusion_model
        elif hasattr(model, 'diffusion_model'):
            print("   æ‰¾åˆ°è·¯å¾„: model.diffusion_model")
            return model.diffusion_model
        elif hasattr(model, 'transformer'):
            print("   æ‰¾åˆ°è·¯å¾„: model.transformer")
            return model.transformer
        else:
            print("   âŒ æ ‡å‡†è·¯å¾„æœªæ‰¾åˆ° transformer")
            
            # è°ƒè¯•ä¿¡æ¯ï¼šåˆ—å‡ºå¯ç”¨å±æ€§
            print("   å¯ç”¨å±æ€§:")
            for attr in dir(model):
                if not attr.startswith('_'):
                    try:
                        obj = getattr(model, attr)
                        if hasattr(obj, '__class__'):
                            print(f"     {attr}: {obj.__class__}")
                    except:
                        pass
            
            return None
    
    def get_stats(self) -> str:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ (å‘åå…¼å®¹)
        
        Returns:
            æ ¼å¼åŒ–çš„ç»Ÿè®¡ä¿¡æ¯å­—ç¬¦ä¸²
        """
        total_calls = self.call_count
        cache_hits = self.skip_count
        avg_compute_time = sum(self.compute_times) / max(len(self.compute_times), 1)
        
        stats = f"""ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯:
æ€» Forward è°ƒç”¨: {total_calls}
ç¼“å­˜å‘½ä¸­: {cache_hits}
ç¼“å­˜å‘½ä¸­ç‡: {cache_hits/max(total_calls,1)*100:.1f}%
å¹³å‡è®¡ç®—æ—¶é—´: {avg_compute_time:.3f}ç§’
é¢„æœŸåŠ é€Ÿæ¯”: {2.0 if cache_hits > 0 else 1.0:.1f}x"""
        
        print(f"\nğŸ“Š {stats}")
        return stats
    
    def get_detailed_stats(self) -> str:
        """
        è·å–è¯¦ç»†ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ (æ–°API)
        
        Returns:
            æ ¼å¼åŒ–çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯å­—ç¬¦ä¸²
        """
        # å…¨å±€ç»Ÿè®¡
        total_calls = self.call_count
        cache_hits = self.skip_count
        avg_compute_time = sum(self.compute_times) / max(len(self.compute_times), 1)
        
        # æŒ‰æ¨¡å‹ç»Ÿè®¡
        model_stats = []
        for model_id, state in self.model_states.items():
            if state.call_count > 0:
                model_avg_time = sum(state.compute_times) / max(len(state.compute_times), 1)
                hit_rate = state.skip_count / max(state.call_count, 1) * 100
                model_stats.append(f"""
  æ¨¡å‹ {model_id[:20]}...:
    è°ƒç”¨æ¬¡æ•°: {state.call_count}
    ç¼“å­˜å‘½ä¸­: {state.skip_count}
    å‘½ä¸­ç‡: {hit_rate:.1f}%
    å¹³å‡è€—æ—¶: {model_avg_time:.3f}s
    ç­–ç•¥: {state.strategy.strategy_type}
    çŠ¶æ€: {'å¯ç”¨' if state.is_enabled else 'ç¦ç”¨'}""")
        
        detailed_stats = f"""=== CacheDiT è¯¦ç»†ç»Ÿè®¡ ===
å…¨å±€ç»Ÿè®¡:
  æ€» Forward è°ƒç”¨: {total_calls}
  æ€»ç¼“å­˜å‘½ä¸­: {cache_hits}
  å…¨å±€å‘½ä¸­ç‡: {cache_hits/max(total_calls,1)*100:.1f}%
  å¹³å‡è®¡ç®—æ—¶é—´: {avg_compute_time:.3f}ç§’
  é¢„æœŸåŠ é€Ÿæ¯”: {2.0 if cache_hits > 0 else 1.0:.1f}x
  æ´»è·ƒæ¨¡å‹æ•°: {len(self.model_states)}

æ¨¡å‹è¯¦æƒ…:{''.join(model_stats) if model_stats else '  æš‚æ— æ´»è·ƒæ¨¡å‹'}"""
        
        print(f"\nğŸ“Š {detailed_stats}")
        return detailed_stats
    
    def get_global_stats(self) -> Dict[str, Any]:
        """
        è·å–å…¨å±€ç»Ÿè®¡ä¿¡æ¯å­—å…¸ (æ–°API)
        
        Returns:
            åŒ…å«è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        return {
            'total_calls': self.call_count,
            'total_cache_hits': self.skip_count,
            'global_hit_rate': self.skip_count / max(self.call_count, 1) * 100,
            'average_compute_time': sum(self.compute_times) / max(len(self.compute_times), 1),
            'expected_speedup': 2.0 if self.skip_count > 0 else 1.0,
            'active_models': len(self.model_states),
            'model_details': {
                model_id: {
                    'calls': state.call_count,
                    'hits': state.skip_count,
                    'hit_rate': state.skip_count / max(state.call_count, 1) * 100,
                    'avg_time': sum(state.compute_times) / max(len(state.compute_times), 1),
                    'strategy': state.strategy.strategy_type,
                    'enabled': state.is_enabled
                }
                for model_id, state in self.model_states.items()
            }
        }
    
    def set_global_config(self, config: Dict[str, Any]):
        """
        è®¾ç½®å…¨å±€é…ç½® (æ–°API)
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.global_config.update(config)
    
    def reset_stats(self):
        """
        é‡ç½®æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯ (æ–°API)
        """
        self.call_count = 0
        self.skip_count = 0
        self.compute_times = []
        
        for state in self.model_states.values():
            state.call_count = 0
            state.skip_count = 0
            state.compute_times = []
    
    # === å‘åå…¼å®¹çš„ç®€å•æ¥å£ ===
    def patch_model_simple(self, model):
        """å‘åå…¼å®¹çš„ç®€å•è¡¥ä¸æ¥å£"""
        return self.patch_model(model)


# å…¨å±€ç¼“å­˜å®ä¾‹ - ä½¿ç”¨å¢å¼ºç‰ˆç¼“å­˜
# ä½¿ç”¨å•ä¾‹æ¨¡å¼ç¡®ä¿æ•´ä¸ª ComfyUI ä¼šè¯ä¸­çš„ä¸€è‡´æ€§
global_cache = EnhancedCache()


# === å‘åå…¼å®¹çš„ç®€å•æ¥å£ ===

def patch_model_simple(model):
    """
    ç®€å•çš„æ¨¡å‹è¡¥ä¸å‡½æ•°ï¼ˆä¿æŒä¸è°ƒè¯•ç‰ˆæœ¬çš„å…¼å®¹æ€§ï¼‰
    
    Args:
        model: ComfyUI æ¨¡å‹å¯¹è±¡
        
    Returns:
        åº”ç”¨äº†ç¼“å­˜çš„æ¨¡å‹
    """
    return global_cache.patch_model(model)


def get_simple_stats():
    """
    è·å–ç®€å•ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¿æŒä¸è°ƒè¯•ç‰ˆæœ¬çš„å…¼å®¹æ€§ï¼‰
    
    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—ç¬¦ä¸²
    """
    return global_cache.get_stats()


# === å…¼å®¹æ€§åˆ«å ===
SimpleCache = EnhancedCache  # å‘åå…¼å®¹